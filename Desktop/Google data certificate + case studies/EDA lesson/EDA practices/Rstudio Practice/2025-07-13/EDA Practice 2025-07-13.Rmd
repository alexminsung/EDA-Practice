---
title: "EDA-Practice-2025-07-13"
output:
  html_document:
    df_print: paged
date: "2025-07-13"
---

### About the Data

The data aims to identify market trends of jobs related to AI and machine learning in 2025 based on the data pulled from 15,000 job listings across 50+ countries. The data is imported from Kaggle ("https://www.kaggle.com/datasets/bismasajjad/global-ai-job-market-and-salary-trends-2025/data") and contains information including - salary, experience level, company size, remote work and more. 

For the purpose of this quick EDA, we will aim to identify any labour market trends related to AI/machine jobs by comparing their salaries, experience expectations and more. 

```{r}
#load libraries used in this EDA
library(skimr)
library(tidyverse)
library(dplyr)
library(ggcorrplot)


#load CSV file.
df <- read.csv("ai_job_dataset.csv")

```

I will start by taking a high-level glimpse of the data using skimr

```{r}
#summarise using skimr()
skim(df)

```
From this output we can identify the data contains 14 categorical columns and 5 numerical columns.
The function also produces distinct summaries based on datatypes, and we can observe that both categorical and numerical columns contain zero missing values (that's great!). ```complete_rate``` simply means the ratio of non-missing values to the total values in the column. ```n_unique``` means the number of unique string values (e.g., ```salary_currency``` only has three distinct values - either USD, GBP or EUR). As for the numerical variables, ```mean``` simply describes the average and ```p50``` indicates the median value. 

Below are the column descriptions provided by the dataset provider.

|         Column         |                          Description                          |
|:----------------------:|:-------------------------------------------------------------:|
| job_id                 | Unique identifier for each job posting                        |
| job_title              | Standardized job title                                        |
| salary_usd             | Annual salary in USD                                          |
| salary_currency        | Original salary currency                                      |
| experience_level       | EN (Entry), MI (Mid), SE (Senior), EX (Executive)             |
| employment_type        | FT (Full-time), PT (Part-time), CT (Contract), FL (Freelance) |
| company_location       | Country where company is located                              |
| company_size           | S (Small <50), M (Medium 50-250), L (Large >250)              |
| employee_residence     | Country where employee resides                                |
| remote_ratio           | 0 (No remote), 50 (Hybrid), 100 (Fully remote)                |
| required_skills        | Top 5 required skills (comma-separated)                       |
| education_required     | Minimum education requirement                                 |
| years_experience       | Required years of experience                                  |
| industry               | Industry sector of the company                                |
| posting_date           | Date when job was posted                                      |
| application_deadline   | Application deadline                                          |
| job_description_length | Character count of job description                            |
| benefits_score         | Numerical score of benefits package (1-10)                    |
| company_name           | Name of the company posting the job advertisement             |

### Data Cleaning and Manipulations

Now that we have a general understanding of the data and columns involved, let's trim down any redundant information. We do have to be little careful how we define "redundant" because what I may find redundant may be considered useful by another. But I think it's safe to assume columns such as ```job_id```,  ```company_name```, and ```job_description_length``` won't provide any helpful information for our EDA. We may regret this later, but we can always add them back eventually. But before we do that, let's check for any duplicate job postings.

```{r}
#check for duplicate postings
#this line of code checks for any duplicate rows (meaning all column values in a given row are identical to another row) in the dataframe. job_id was purposely omitted because it is possible for two separate postings to exist (thus unique job_id) containing duplicate information. 
duplicate_check <- df %>% select (!job_id) %>% duplicated() %>% sum()
print(duplicate_check)


#remove job_id and company_name
df <- df  %>% select(-job_id, - company_name, -job_description_length)

#let's re-level some of the ordinal variables in ascending order
df$experience_level <- factor(df$experience_level, levels = c("EN", "MI", "SE", "EX"))
df$company_size <- factor(df$company_size, levels = c("S", "M", "L"))


#let's relabel the numerical variables
df <- df %>% rename("Salary" = salary_usd, "Remote Ratio" = remote_ratio, "Years of Experience" = years_experience, 
                    "Benefits Score" = benefits_score)
```
As we can see from our little line of code above, we see that there are no duplicate postings. Therefore, we will remove job_id and company name from the data. Luckily for us, the data is very clean and contains no missing values. 

### Key Visualizations

Now that we have ourselves a 'relatively' clean data, let's explore some distributions. 

```{r}
#count distribution of numeric data (e.g., histogram)
#pivot_longer function with cols = 1:4 deconstructs each column value associated with a given row and produces a distinct row value containing only the said column value and the column name as its "feature". You can think of it like transposing the column values to the row.
#I chose cols = 1:4 because there are 4 columns in total so each column value will get its own row. There are other ways to play around with the cols parameter. You can refer to this link for more information and examples (https://cran.r-project.org/web/packages/tidyr/vignettes/pivot.html)
df_numeric <- df %>% select(where(is.numeric)) %>% pivot_longer(cols = 1:4, names_to = "feature", values_to = "values")

#scales = "free" allows for individual plots to have their own binwidth
df_numeric %>% ggplot(aes(x=values)) + geom_histogram() + facet_wrap(~feature, scales = "free")

```

Based on the 4 histograms above, we can deduce the following information. First of all, the obvious elephant in the room is the remote_ratio plot which appears uniform across the 3 work models. More importantly, it's obvious that we should have used a bar plot instead of a histogram since the variable only has 3 options (0(not remote), 50(hybrid), 100(fully remote)). So in reality, remote_ratio should have been treated like a discrete, nominal variable. Secondly, we observe that the salary (in USD) tends to gather under the 100,000 dollar mark, meaning most of the job postings were advertised 100k and under. This is a useful piece of information to understand what the general salary looks like in the current labour market. If we review our initial summary of the data, we also see that the salary average is 115,349 and median is 99,705 which is consistent to our histogram. We also see that most postings generally require 0 to 5 years of experience, but a good number of the postings offer no years of experience (hooray!). 

```{r}
#plotting a bar graph of years in industry
df %>% ggplot(aes(x=`Years of Experience`)) + geom_bar()
```

We can also use a bar plot to observe individual count per "year of require experience" wherein we see that a good number of postings only require 0 to 1 year of experience. Reviewing our earlier summary, we can see that on average a candidate would need 6 years of experience (and 5 going by the median). 

Finally, ```benefits_score``` appear to have a multinomial distribution but I genuinely don't know what benefits_score is meant to represent so I will ignore this variable moving forward.  

We explored the individual distributions of the continuous variables, so let's see how they compare to one another. First, let's use a correlation matrix to determine the relationship between numerical variables. 

```{r}
#plot a correlation matrix of relevant continous variables
#in essence, the sapply(df, is.numeric) extracts the columns that contain numeric values and uses them as index for the initial dataframe. By calling these indices, we only target numerical values wherein they are used to calculate correlations. We use ggcorrplot to plot the correlation matrix.
cor(df[sapply(df, is.numeric)]) %>% ggcorrplot(lab = TRUE, legend.title = "Correlation")

```
From looking at the correlation matrix we see only two variables are strongly correlated: years of experience and salary. Now this makes intuitive sense because one would expect their salary to increase as you move up the corporate ladder. So why don't we take a closer look at these two variables? 

```{r}
#plot a series of boxplots to explore the relationship between two years of experience and salary
df %>% ggplot() + geom_boxplot(aes(x = as.factor(years_experience), y = salary_usd)) + labs(x = "Years of Experience", y = "Salary(USD)")

```
I decided to use a series of box plots to determine individual distributions of salary for each year of experience. Not only does this allow me to observe the median and outliers, this plot makes more sense than a scatter plot since ```years_experience``` is an integer variable and hence a scatter plot would've had a lot of overlapping vertical lines. From looking at the graph above, we see a general upward shift until 10 years where it maintains a consistent salary range. However, outliers become much more prevalent from 10+ years of experience hinting that salary may largely depend on the industry and the company. In general, we observe higher variability past the 75% percentile for all box plots indicating a right-skewed distribution across all level of experience. This may allude to possible covariates in play which may influence higher variability past the 75% percentile. 

We explored the distribution of numerical variables, so let's switch gears and explore how some of their categorical cousins look like. Firstly, I'm interested to know the experience level of the 15,000 jobs. This will help us explain if the market is in demand for entry positions or if it is looking for more experienced programmers. 

```{r}
#general distribution of the required experience level
df %>% ggplot() + geom_bar(aes(x = experience_level)) + labs(x = "Experience Level")

#a more detailed look of the distribution taking the employment type into consideration
df %>% ggplot() + geom_bar(aes(x = experience_level, fill = employment_type), position = "dodge") + labs(x = "Experience Level", fill = "Employment Type")
```
Upon inspection, we realize the labour market doesn't seem to favor a particular experience level. However, when we take the employment type into consideration, we observe the following: 

* Entry level positions offer more part-time opportunities than other experience levels

* There is a surge of freelance openings for mid level positions than other experience levels

* There is a higher number of full-time openings for senior level positions

* Executive level positions see a higher number of contract-oriented opportunities than other experience levels

Keep in mind employment types still remain fairly consistent across the board regardless of the experience level. 

Let's observe some more relationships.

```{r}
#I'm curious to know the relationship between company size and required experience
df %>% ggplot() + geom_bar(aes(x = company_size, fill = experience_level), position = "dodge") 

```

Another interesting consideration would be to explore the relationship between company size and experience level. Specifically, we may want to know if the size of the company solicit a specific experience level. Although the initial observation suggest yet another even spread between experience levels, it does seem like smaller companies prefer entry/executive positions, larger companies prefer mid-level or senior positions, and mid-sized companies enjoy a healthy dose of mid-level programmers. 

### Future Considerations

In addition to everything described in this report, the following steps may help identify additional trends or information:

* Verify the reliability of the data - is the data from a credible source/organization? Is it original and up-to-date?

### Citations

Skimr defaults - https://cran.r-project.org/web/packages/skimr/vignettes/Skimr_defaults.html
